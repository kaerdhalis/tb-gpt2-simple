{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setting things up\n",
    "The following cell install all the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==0.25.3 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (0.25.3)\n",
      "Requirement already satisfied: numpy==1.17.3 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.17.3)\n",
      "Requirement already satisfied: tensorflow==1.15.2 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.15.2)\n",
      "Requirement already satisfied: gpt-2-simple in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from pandas==0.25.3->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from pandas==0.25.3->-r requirements.txt (line 1)) (2020.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.0.8)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.12.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.29.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.15.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (0.34.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.1.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (0.8.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (3.12.2)\n",
      "Requirement already satisfied: regex in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from gpt-2-simple->-r requirements.txt (line 4)) (2020.6.8)\n",
      "Requirement already satisfied: requests in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from gpt-2-simple->-r requirements.txt (line 4)) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from gpt-2-simple->-r requirements.txt (line 4)) (4.46.1)\n",
      "Requirement already satisfied: toposort in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from gpt-2-simple->-r requirements.txt (line 4)) (1.5)\n",
      "Requirement already satisfied: h5py in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.2->-r requirements.txt (line 3)) (2.10.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 3)) (47.1.1.post20200604)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 3)) (3.2.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from requests->gpt-2-simple->-r requirements.txt (line 4)) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from requests->gpt-2-simple->-r requirements.txt (line 4)) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from requests->gpt-2-simple->-r requirements.txt (line 4)) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from requests->gpt-2-simple->-r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 3)) (1.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 3)) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Import the packages needed for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import requests\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import csv\n",
    "import zipfile\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s [%(levelname)-5.5s] [%(name)-12.12s]: %(message)s')\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "run_name='run2'\n",
    "data_path ='data/shakespeare.txt'\n",
    "steps=1\n",
    "length = 600 \n",
    "temperature = 0.7\n",
    "top_k = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We define a helper function for zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "block:load_data_helper"
    ]
   },
   "outputs": [],
   "source": [
    "# helper to use code samples in zip file\n",
    "def process_zip(name, regs, postfix,data_dir,min_length,max_length,preserve_form,num_samples):\n",
    "    with open(os.path.join(output_dir, name + postfix + '.txt'), 'w+') as fh:\n",
    "        with zipfile.ZipFile(os.path.join(data_dir, name + '.zip'), 'r') as z:\n",
    "            cnt = 0\n",
    "            for entry in z.namelist():\n",
    "                text = z.read(entry).decode('utf-8')\n",
    "                for reg, sub in regs.items():\n",
    "                    text = re.sub(reg, sub, text, flags=re.DOTALL)\n",
    "                if len(text) > min_length and len(text) <= max_length:\n",
    "                    sample = text.strip() + \"\\n\"\n",
    "                    if preserve_form == 'true':\n",
    "                        sample += \"\\n\\n\"\n",
    "                    fh.write(sample)\n",
    "                    cnt += 1\n",
    "                if cnt >= num_samples:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The next cells will prepare the data sets we can use in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# command line arguments parser\n",
    "data_type='all'\n",
    "data_dir = './datasets/'\n",
    "output_dir = './data'\n",
    "short_filename = 'true'\n",
    "postfix = ''\n",
    "num_samples = 1000\n",
    "max_length = 2000\n",
    "min_length = 10\n",
    "preserve_lines = 'true'\n",
    "preserve_form = 'false'\n",
    "\n",
    "# form requires newlines to be preserved\n",
    "if preserve_form == 'true':\n",
    "    preserve_lines = 'true'\n",
    "\n",
    "# collapsing sample into one line requires form not to be preserved\n",
    "if preserve_lines == 'false':\n",
    "    preserve_form = 'false'\n",
    "\n",
    "# set postfix for output files if short-filename is false\n",
    "if postfix != '':\n",
    "    postfix = '_' + postfix\n",
    "if short_filename == 'false':\n",
    "    postfix += f'_n{num_samples}_min{min_length}_max{max_length}'\n",
    "    if preserve_lines == 'false':\n",
    "        postfix += '_nolines'\n",
    "    else:\n",
    "        postfix += '_lines'\n",
    "    if preserve_form == 'false':\n",
    "        postfix += '_noform'\n",
    "    else:\n",
    "        postfix += '_form'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare tweet data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "block:load_tweet_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare tweet data set...\n",
      "preparing tweet data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FKJEBIL\n",
    "if data_type in ['all','tweets']: # parse trump tweets\n",
    "    print('prepare tweet data set...')\n",
    "    df1 = pd.read_json(os.path.join(data_dir, 'realdonaldtrump-1.ndjson'), lines=True)\n",
    "    df2 = pd.read_json(os.path.join(data_dir, 'realdonaldtrump-2.ndjson'), lines=True)\n",
    "    df = pd.concat([df1, df2], sort=True)\n",
    "    if preserve_lines == 'false':\n",
    "        df.text = df.text.str.replace(\"\\n\",\" \")\n",
    "    if preserve_form == 'false':\n",
    "        df.text = df.text.str.replace(r\"https?://[^\\s]+\",\"\")\n",
    "    df['length'] = df.text.apply(len)\n",
    "    filter = (df.text>'2017')&(df.text.str.startswith('RT')==False)&(df.length>min_length)\n",
    "    df = df[filter]\n",
    "    df.sample(num_samples).text.to_csv(os.path.join(output_dir, 'tweets' + postfix + '.txt'), index=False, header=False, quoting=csv.QUOTE_NONE, escapechar=\"\\\\\", sep=\"\\\\\")\n",
    "    print('preparing tweet data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare chess data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "block:load_chess_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare chess data set...\n",
      "preparing chess data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: https://www.ficsgames.org/download.html | year: 2019, month: whole year, type: Standard (average rating > 2000)\n",
    "if data_type in ['all','chess']: # parse chess games\n",
    "    print('prepare chess data set...')\n",
    "    with open(os.path.join(output_dir, 'chess' + postfix + '.txt'),'w+') as fh:\n",
    "        with open(os.path.join(data_dir, 'ficsgamesdb_2019_standard2000_nomovetimes_110541.pgn')) as fp:\n",
    "           line = fp.readline()\n",
    "           cnt = 0\n",
    "           while line and cnt < num_samples:\n",
    "               if line.startswith('1.'):\n",
    "                   fh.write(line)\n",
    "                   cnt += 1\n",
    "               line = fp.readline()\n",
    "    print('preparing chess data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare music data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "block:load_music_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare music data set...\n",
      "preparing music data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: https://www.kaggle.com/raj5287/abc-notation-of-tunes/version/3\n",
    "if data_type in ['all','music']: # parse abc songs\n",
    "    print('prepare music data set...')\n",
    "    with open(os.path.join(output_dir, 'music' + postfix + '.txt'),'w+') as fh:\n",
    "        with open(os.path.join(data_dir, 'abc_notation_songs.txt')) as fp:\n",
    "            line = fp.readline()\n",
    "            cnt = 0\n",
    "            song = \"\"\n",
    "            while line and cnt < num_samples:\n",
    "                if len(line) < 2 or line[1:2] == ':':\n",
    "                    if song != \"\":\n",
    "                        fh.write(song + \"\\n\")\n",
    "                        cnt += 1\n",
    "                        song = \"\"\n",
    "                elif preserve_lines == 'false':\n",
    "                    song += \" \" + line.strip()\n",
    "                else:\n",
    "                    fh.write(line.strip() + \"\\n\")\n",
    "                line = fp.readline()\n",
    "    print('preparing music data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare shakespeare data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "block:load_shakespeare_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare shakespeare data set...\n",
      "preparing shakespeare data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: https://www.kaggle.com/kingburrito666/shakespeare-plays\n",
    "if data_type in ['all','shakespeare']: # parse shakespeare plays\n",
    "    print('prepare shakespeare data set...')\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'shakespeare_data.csv'))\n",
    "    if preserve_lines == 'false':\n",
    "        df = df[df.Player!=''].groupby(['Play','PlayerLinenumber'],as_index=False).agg(' '.join)\n",
    "    df.sample(num_samples).PlayerLine.to_csv(os.path.join(output_dir, 'shakespeare' + postfix + '.txt'), index=False, header=False, quoting=csv.QUOTE_NONE, escapechar=\"\\\\\", sep=\"\\\\\")\n",
    "    print('preparing shakespeare data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare javascript data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "block:load_javascript_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare javascript data set...\n",
      "preparing javascript data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: javascript files from https://www.sri.inf.ethz.ch/js150\n",
    "if data_type in ['all','javascript']: # parse javascript files\n",
    "    print('prepare javascript data set...')\n",
    "    regexes = {}\n",
    "    if preserve_form == 'false':\n",
    "        regexes[r'(//[^\\n]*)?\\n|/\\*.*?\\*/'] = '\\n'\n",
    "        regexes[r'\\n\\s*\\n'] = '\\n'\n",
    "    if preserve_lines == 'false':\n",
    "        regexes[r'\\s+'] = ' '\n",
    "    process_zip('javascript', regexes, postfix,data_dir,min_length,max_length,preserve_form,num_samples)\n",
    "    print('preparing javascript data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare typescript data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "block:load_typescript_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare typescript data set...\n",
      "preparing typescript data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: typescript files collected from standard angular app\n",
    "if data_type in ['all','typescript']: # parse typescript files\n",
    "    print('prepare typescript data set...')\n",
    "    regexes = {}\n",
    "    if preserve_form == 'false':\n",
    "        regexes[r'(//[^\\n]*)?\\n|/\\*.*?\\*/'] = '\\n'\n",
    "        regexes[r'\\n\\s*\\n'] = '\\n'\n",
    "    if preserve_lines == 'false':\n",
    "        regexes[r'\\s+'] = ' '\n",
    "    process_zip('typescript', regexes, postfix,data_dir,min_length,max_length,preserve_form,num_samples)\n",
    "    print('preparing typescript data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare json data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "block:load_json_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare json data set...\n",
      "preparing json data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: json files collected from standard angular app\n",
    "if data_type in ['all','json']: # parse json files\n",
    "    print('prepare json data set...')\n",
    "    regexes = {}\n",
    "    if preserve_lines == 'false':\n",
    "        regexes[r'\\s+'] = ' '\n",
    "    process_zip('json', regexes, postfix,data_dir,min_length,max_length,preserve_form,num_samples)\n",
    "    print('preparing json data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare html data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "block:load_html_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare html data set...\n",
      "preparing html data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: https://www.kaggle.com/zavadskyy/lots-of-code, https://gist.github.com/VladislavZavadskyy/e31ab07b03a5c22b11982c49669a400b\n",
    "if data_type in ['all','html']: # parse html\n",
    "    print('prepare html data set...')\n",
    "    with open(os.path.join(output_dir, 'html' + postfix + '.txt'),'w+') as fh:\n",
    "        with open(os.path.join(data_dir, 'html-dataset.txt')) as fp:\n",
    "            data = fp.read()\n",
    "            data = data.replace('<!DOCTYPE html>','\\n<!DOCTYPE html>')\n",
    "            lines = data.split('\\n')\n",
    "            cnt = 0\n",
    "            sample = \"\"\n",
    "            for line in lines:\n",
    "                if line == \"\":\n",
    "                    continue\n",
    "                if sample != \"\" and line.startswith('<!DOCTYPE html>'):\n",
    "                    fh.write(sample.strip() + \"\\n\")\n",
    "                    sample = \"\"\n",
    "                    cnt += 1\n",
    "                if cnt >= num_samples:\n",
    "                    break\n",
    "                line = re.sub(r'\\s+', ' ', line)\n",
    "                sample += line.strip() + \" \"\n",
    "    print('preparing html data set done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Let's fine-tune the GPT-2 model!\n",
    "Choose the number of steps the model will be fine-tuned for. You can adjust the parameters  to specifiy how often you get updates on the training process, how often samples of the current model are printed, and every how many steps the model is saved.\n",
    "\n",
    "Beside the number of steps, these parameters do not influence the training. The model will be saved automatically when done fine-tuning with the amount of steps specified. You can stop the fine-tuning anytime and the current training state of the model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "block:finetune_model",
     "prev:load_music_data"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run fine-tuning for run run2 using GPT2 model 124M...\n",
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-13 12:08:32,493 [WARNI] [tensorflow  ]: From /home/jovyan/.local/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-13 12:08:56,607 [INFO ] [tensorflow  ]: Restoring parameters from models/124M/model.ckpt\n",
      "100%|██████████| 1/1 [00:00<00:00, 717.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "dataset has 11351 tokens\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 | 69.35] loss=4.61 avg=4.61\n",
      "[2 | 125.39] loss=4.61 avg=4.61\n",
      "[3 | 182.37] loss=4.39 avg=4.54\n",
      "[4 | 238.20] loss=4.44 avg=4.51\n",
      "[5 | 292.90] loss=4.27 avg=4.46\n",
      "[6 | 347.93] loss=4.36 avg=4.45\n",
      "[7 | 401.99] loss=4.18 avg=4.41\n",
      "[8 | 456.62] loss=4.24 avg=4.38\n",
      "[9 | 511.50] loss=4.03 avg=4.34\n",
      "[10 | 567.92] loss=4.06 avg=4.31\n",
      "Saving runs/run2/model-10\n"
     ]
    }
   ],
   "source": [
    "def start_session(sess):\n",
    "    try:\n",
    "        gpt2.reset_session(sess)\n",
    "    except:\n",
    "        pass\n",
    "    return gpt2.start_tf_sess()\n",
    "\n",
    "def fine_tune(sess,run_name,data_path,steps, model_name='124M'):\n",
    "    print(f'Run fine-tuning for run {run_name} using GPT2 model {model_name}...')\n",
    "    if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "        log.info(f\"Downloading {model_name} model...\")\n",
    "        gpt2.download_gpt2(model_name=model_name)\n",
    "    sess = start_session(sess)\n",
    "    gpt2.finetune(sess=sess,dataset=data_path,checkpoint_dir='runs', model_name=model_name, run_name=run_name, steps=steps, sample_every=10, save_every=10)\n",
    "\n",
    "\n",
    "\n",
    "#run_name='run1'\n",
    "#data_path ='data/tweets.txt'\n",
    "sess = None\n",
    "fine_tune(sess,run_name,data_path,steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text generation\n",
    "We can now generate text mimiking the style of the learned samples.\n",
    "\n",
    "You can play around with the three parameters `length`, `temperature`, and `top_k` to influnce the generated text. Further, you can provide a seed sequence that will be the beginning of the generated text.\n",
    "\n",
    "Use the different data sets to explore how the fine-tuning works and what its' limits are. You can also use custom data sets. Just copy them to the data folder and specify the path above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "block:laod_score",
     "prev:finetune_model"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-13 12:23:59,630 [INFO ] [pytorch_pret]: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "2020-07-13 12:23:59,642 [DEBUG] [urllib3.conn]: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-07-13 12:24:00,047 [DEBUG] [urllib3.conn]: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/openai-gpt-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "2020-07-13 12:24:00,052 [DEBUG] [urllib3.conn]: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-07-13 12:24:00,439 [DEBUG] [urllib3.conn]: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/openai-gpt-config.json HTTP/1.1\" 200 0\n",
      "2020-07-13 12:24:00,444 [INFO ] [pytorch_pret]: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin from cache at /home/jovyan/.pytorch_pretrained_bert/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n",
      "2020-07-13 12:24:00,445 [INFO ] [pytorch_pret]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json from cache at /home/jovyan/.pytorch_pretrained_bert/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.bd0797be126548711309ad2174d2afb16e3c37e891707667603d85e35a4ad001\n",
      "2020-07-13 12:24:00,448 [INFO ] [pytorch_pret]: Model config {\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "2020-07-13 12:24:03,340 [DEBUG] [urllib3.conn]: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-07-13 12:24:03,729 [DEBUG] [urllib3.conn]: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/openai-gpt-vocab.json HTTP/1.1\" 200 0\n",
      "2020-07-13 12:24:03,735 [DEBUG] [urllib3.conn]: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-07-13 12:24:04,125 [DEBUG] [urllib3.conn]: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/openai-gpt-merges.txt HTTP/1.1\" 200 0\n",
      "2020-07-13 12:24:04,128 [INFO ] [pytorch_pret]: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json from cache at /home/jovyan/.pytorch_pretrained_bert/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
      "2020-07-13 12:24:04,129 [INFO ] [pytorch_pret]: loading merges file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt from cache at /home/jovyan/.pytorch_pretrained_bert/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
      "2020-07-13 12:24:04,132 [WARNI] [pytorch_pret]: ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "model.eval()\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "def score(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    loss=model(tensor_input, lm_labels=tensor_input)\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "block:generate_text",
     "prev:finetune_model",
     "prev:laod_score"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint runs/run2/model-10\n",
      "INFO:tensorflow:Restoring parameters from runs/run2/model-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-13 12:24:10,019 [INFO ] [tensorflow  ]: Restoring parameters from runs/run2/model-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social justice warior, and the greatest of the Missouri, the most valiant, the most valiant, and the most valiant.\n",
      "\n",
      "We have now come to the place where the troops have come, and we must put them to death.\n",
      "\n",
      "This is the word of the Lord, and if there be not a hundred to be made of this body,\n",
      "\n",
      "but the most noble, the most fattening, the most gallant, the most bold.\n",
      "\n",
      "And now, sir, let our red-hot fire stand, and I'll throw it into the river,\n",
      "\n",
      "And go, my lord, get back your head.\n",
      "\n",
      "And I know not what.\n",
      "\n",
      "And so, in a day or two, and then, when the scepter has been cast,\n",
      "\n",
      "Come, let's talk.\n",
      "\n",
      "I'll tell you, sir, the state, and I'll tell you,\n",
      "\n",
      "And I know not how to fly into this and that.\n",
      "\n",
      "We have in his hand, I know not how to make myself.\n",
      "\n",
      "Come, let's get you some good news, and say, 'I have a letter from you,\n",
      "\n",
      "This is, I get you some rest, and you go,\n",
      "\n",
      "for if my father's the king, he'll make his peace.'\n",
      "\n",
      "And, though it be true, I'd rather take him as a friend\n",
      "\n",
      "That's not in consideration.\n",
      "\n",
      "And if I saw him, I would stop him.\n",
      "\n",
      "Should you be pleased?\n",
      "\n",
      "And if you shall do so,\n",
      "\n",
      "I will make my peace with you.\n",
      "\n",
      "To know this, sir, is a strong thing to do.\n",
      "\n",
      "And, being now in this state,\n",
      "\n",
      "To be turbulent, and to be cast down, and that,\n",
      "\n",
      "If any one should say as to that,\n",
      "\n",
      "I'll be there in the same place, and he'll be gone.\n",
      "\n",
      "For as in the state,\n",
      "\n",
      "The day of my death, sir, is gone,\n",
      "\n",
      "For, if it be not so,\n",
      "\n",
      "And for that which the state objecteth,\n",
      "\n",
      "To make me mad, sir, and my hope,\n",
      "\n",
      "To be to the house, and to their distant woods to be thine.\n",
      "\n",
      "All the rest, go away,\n",
      "\n",
      "I'll do no more, and if it be not so,\n",
      "\n",
      "To get to the point.\n",
      "\n",
      "And yet, I must say, he is a good man,\n",
      "\n",
      "Whom, my lord, I am sure he is,\n",
      "\n",
      "But, if you let him live,\n",
      "\n",
      "You know, he's dead.\n",
      "\n",
      "And, for a while, I'll yet see him,\n",
      "\n",
      "And, if it be not so,\n",
      "\n",
      "If I could be of a kind, I would have it.\n",
      "\n",
      "Come now, have a look\n",
      "-----------------------------\n",
      "28.224601377103017\n"
     ]
    }
   ],
   "source": [
    "def generate(sess,run_name,length,temperature,top_k):\n",
    "\n",
    "    message = \"Social justice warior\"\n",
    "    text = gpt2.generate(sess=sess,checkpoint_dir='runs', run_name=run_name, prefix=message, length=length, temperature=temperature, top_k=top_k, return_as_list=True)\n",
    "    print(text[0])\n",
    "    print(\"-----------------------------\")\n",
    "    print(score(text[0][0:length-10]))\n",
    "    \n",
    "    metrics = {\n",
    "    'metrics': [{\n",
    "      'name': 'perplexity-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':score(text[0][0:length-10]), # The value of the metric. Must be a numeric value.\n",
    "      'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "      }]\n",
    "    }\n",
    "    with file_io.FileIO('/mlpipeline-metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "    \n",
    "    \n",
    "    \n",
    "#length = 800 # { min:0, max:1000, step:5}\n",
    "#temperature = 0.7 # { min:0, max:2, step:0.1}\n",
    "#top_k = 0\n",
    "sess = start_session(sess)\n",
    "gpt2.load_gpt2(sess, checkpoint_dir='runs', run_name=run_name)\n",
    "generate(sess,run_name,length,temperature,top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "docker_image": "gcr.io/arrikto-public/tensorflow-1.15.2-notebook-cpu:1.0.0.arr1",
   "experiment": {
    "id": "17d9eb89-1f16-4ab5-bc06-9c84d5550990",
    "name": "Gpt2-simple"
   },
   "experiment_name": "Gpt2-simple",
   "pipeline_description": "finetuning basic",
   "pipeline_name": "finetune-pipeline",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "workspace-tb-finetuning-eali2s34e",
     "size": 8,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
