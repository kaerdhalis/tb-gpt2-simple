{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setting things up\n",
    "The following cell install all the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==0.25.3 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (0.25.3)\n",
      "Requirement already satisfied: numpy==1.17.3 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.17.3)\n",
      "Requirement already satisfied: tensorflow==1.15.2 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.15.2)\n",
      "Requirement already satisfied: gpt-2-simple in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from pandas==0.25.3->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from pandas==0.25.3->-r requirements.txt (line 1)) (2020.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.0.8)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.12.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.29.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.15.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (0.34.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.1.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (0.8.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 3)) (3.12.2)\n",
      "Requirement already satisfied: regex in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from gpt-2-simple->-r requirements.txt (line 4)) (2020.6.8)\n",
      "Requirement already satisfied: requests in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from gpt-2-simple->-r requirements.txt (line 4)) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from gpt-2-simple->-r requirements.txt (line 4)) (4.46.1)\n",
      "Requirement already satisfied: toposort in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from gpt-2-simple->-r requirements.txt (line 4)) (1.5)\n",
      "Requirement already satisfied: h5py in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.2->-r requirements.txt (line 3)) (2.10.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 3)) (47.1.1.post20200604)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 3)) (3.2.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from requests->gpt-2-simple->-r requirements.txt (line 4)) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from requests->gpt-2-simple->-r requirements.txt (line 4)) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from requests->gpt-2-simple->-r requirements.txt (line 4)) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from requests->gpt-2-simple->-r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 3)) (1.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/benjamin/miniconda3/envs/tb-propre-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 3)) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Import the packages needed for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import requests\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import csv\n",
    "import zipfile\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s [%(levelname)-5.5s] [%(name)-12.12s]: %(message)s')\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "run_name='run1'\n",
    "data_path ='data/tweets.txt'\n",
    "steps=1\n",
    "length = 800 \n",
    "temperature = 0.7\n",
    "top_k = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We define a helper function for zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "block:load_data_helper"
    ]
   },
   "outputs": [],
   "source": [
    "# helper to use code samples in zip file\n",
    "def process_zip(name, regs, postfix,data_dir,min_length,max_length,preserve_form,num_samples):\n",
    "    with open(os.path.join(output_dir, name + postfix + '.txt'), 'w+') as fh:\n",
    "        with zipfile.ZipFile(os.path.join(data_dir, name + '.zip'), 'r') as z:\n",
    "            cnt = 0\n",
    "            for entry in z.namelist():\n",
    "                text = z.read(entry).decode('utf-8')\n",
    "                for reg, sub in regs.items():\n",
    "                    text = re.sub(reg, sub, text, flags=re.DOTALL)\n",
    "                if len(text) > min_length and len(text) <= max_length:\n",
    "                    sample = text.strip() + \"\\n\"\n",
    "                    if preserve_form == 'true':\n",
    "                        sample += \"\\n\\n\"\n",
    "                    fh.write(sample)\n",
    "                    cnt += 1\n",
    "                if cnt >= num_samples:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The next cells will prepare the data sets we can use in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# command line arguments parser\n",
    "data_type='all'\n",
    "data_dir = './datasets/'\n",
    "output_dir = './data'\n",
    "short_filename = 'true'\n",
    "postfix = ''\n",
    "num_samples = 1000\n",
    "max_length = 2000\n",
    "min_length = 10\n",
    "preserve_lines = 'true'\n",
    "preserve_form = 'false'\n",
    "\n",
    "# form requires newlines to be preserved\n",
    "if preserve_form == 'true':\n",
    "    preserve_lines = 'true'\n",
    "\n",
    "# collapsing sample into one line requires form not to be preserved\n",
    "if preserve_lines == 'false':\n",
    "    preserve_form = 'false'\n",
    "\n",
    "# set postfix for output files if short-filename is false\n",
    "if postfix != '':\n",
    "    postfix = '_' + postfix\n",
    "if short_filename == 'false':\n",
    "    postfix += f'_n{num_samples}_min{min_length}_max{max_length}'\n",
    "    if preserve_lines == 'false':\n",
    "        postfix += '_nolines'\n",
    "    else:\n",
    "        postfix += '_lines'\n",
    "    if preserve_form == 'false':\n",
    "        postfix += '_noform'\n",
    "    else:\n",
    "        postfix += '_form'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare tweet data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "block:load_tweet_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare tweet data set...\n",
      "preparing tweet data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FKJEBIL\n",
    "if data_type in ['all','tweets']: # parse trump tweets\n",
    "    print('prepare tweet data set...')\n",
    "    df1 = pd.read_json(os.path.join(data_dir, 'realdonaldtrump-1.ndjson'), lines=True)\n",
    "    df2 = pd.read_json(os.path.join(data_dir, 'realdonaldtrump-2.ndjson'), lines=True)\n",
    "    df = pd.concat([df1, df2], sort=True)\n",
    "    if preserve_lines == 'false':\n",
    "        df.text = df.text.str.replace(\"\\n\",\" \")\n",
    "    if preserve_form == 'false':\n",
    "        df.text = df.text.str.replace(r\"https?://[^\\s]+\",\"\")\n",
    "    df['length'] = df.text.apply(len)\n",
    "    filter = (df.text>'2017')&(df.text.str.startswith('RT')==False)&(df.length>min_length)\n",
    "    df = df[filter]\n",
    "    df.sample(num_samples).text.to_csv(os.path.join(output_dir, 'tweets' + postfix + '.txt'), index=False, header=False, quoting=csv.QUOTE_NONE, escapechar=\"\\\\\", sep=\"\\\\\")\n",
    "    print('preparing tweet data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare chess data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "block:load_chess_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare chess data set...\n",
      "preparing chess data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: https://www.ficsgames.org/download.html | year: 2019, month: whole year, type: Standard (average rating > 2000)\n",
    "if data_type in ['all','chess']: # parse chess games\n",
    "    print('prepare chess data set...')\n",
    "    with open(os.path.join(output_dir, 'chess' + postfix + '.txt'),'w+') as fh:\n",
    "        with open(os.path.join(data_dir, 'ficsgamesdb_2019_standard2000_nomovetimes_110541.pgn')) as fp:\n",
    "           line = fp.readline()\n",
    "           cnt = 0\n",
    "           while line and cnt < num_samples:\n",
    "               if line.startswith('1.'):\n",
    "                   fh.write(line)\n",
    "                   cnt += 1\n",
    "               line = fp.readline()\n",
    "    print('preparing chess data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare music data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "block:load_music_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare music data set...\n",
      "preparing music data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: https://www.kaggle.com/raj5287/abc-notation-of-tunes/version/3\n",
    "if data_type in ['all','music']: # parse abc songs\n",
    "    print('prepare music data set...')\n",
    "    with open(os.path.join(output_dir, 'music' + postfix + '.txt'),'w+') as fh:\n",
    "        with open(os.path.join(data_dir, 'abc_notation_songs.txt')) as fp:\n",
    "            line = fp.readline()\n",
    "            cnt = 0\n",
    "            song = \"\"\n",
    "            while line and cnt < num_samples:\n",
    "                if len(line) < 2 or line[1:2] == ':':\n",
    "                    if song != \"\":\n",
    "                        fh.write(song + \"\\n\")\n",
    "                        cnt += 1\n",
    "                        song = \"\"\n",
    "                elif preserve_lines == 'false':\n",
    "                    song += \" \" + line.strip()\n",
    "                else:\n",
    "                    fh.write(line.strip() + \"\\n\")\n",
    "                line = fp.readline()\n",
    "    print('preparing music data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare shakespeare data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "block:load_shakespeare_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare shakespeare data set...\n",
      "preparing shakespeare data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: https://www.kaggle.com/kingburrito666/shakespeare-plays\n",
    "if data_type in ['all','shakespeare']: # parse shakespeare plays\n",
    "    print('prepare shakespeare data set...')\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'shakespeare_data.csv'))\n",
    "    if preserve_lines == 'false':\n",
    "        df = df[df.Player!=''].groupby(['Play','PlayerLinenumber'],as_index=False).agg(' '.join)\n",
    "    df.sample(num_samples).PlayerLine.to_csv(os.path.join(output_dir, 'shakespeare' + postfix + '.txt'), index=False, header=False, quoting=csv.QUOTE_NONE, escapechar=\"\\\\\", sep=\"\\\\\")\n",
    "    print('preparing shakespeare data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare javascript data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "block:load_javascript_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare javascript data set...\n",
      "preparing javascript data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: javascript files from https://www.sri.inf.ethz.ch/js150\n",
    "if data_type in ['all','javascript']: # parse javascript files\n",
    "    print('prepare javascript data set...')\n",
    "    regexes = {}\n",
    "    if preserve_form == 'false':\n",
    "        regexes[r'(//[^\\n]*)?\\n|/\\*.*?\\*/'] = '\\n'\n",
    "        regexes[r'\\n\\s*\\n'] = '\\n'\n",
    "    if preserve_lines == 'false':\n",
    "        regexes[r'\\s+'] = ' '\n",
    "    process_zip('javascript', regexes, postfix,data_dir,min_length,max_length,preserve_form,num_samples)\n",
    "    print('preparing javascript data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare typescript data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "block:load_typescript_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare typescript data set...\n",
      "preparing typescript data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: typescript files collected from standard angular app\n",
    "if data_type in ['all','typescript']: # parse typescript files\n",
    "    print('prepare typescript data set...')\n",
    "    regexes = {}\n",
    "    if preserve_form == 'false':\n",
    "        regexes[r'(//[^\\n]*)?\\n|/\\*.*?\\*/'] = '\\n'\n",
    "        regexes[r'\\n\\s*\\n'] = '\\n'\n",
    "    if preserve_lines == 'false':\n",
    "        regexes[r'\\s+'] = ' '\n",
    "    process_zip('typescript', regexes, postfix,data_dir,min_length,max_length,preserve_form,num_samples)\n",
    "    print('preparing typescript data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare json data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "block:load_json_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare json data set...\n",
      "preparing json data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: json files collected from standard angular app\n",
    "if data_type in ['all','json']: # parse json files\n",
    "    print('prepare json data set...')\n",
    "    regexes = {}\n",
    "    if preserve_lines == 'false':\n",
    "        regexes[r'\\s+'] = ' '\n",
    "    process_zip('json', regexes, postfix,data_dir,min_length,max_length,preserve_form,num_samples)\n",
    "    print('preparing json data set done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prepare html data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "block:load_html_data",
     "prev:load_data_helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare html data set...\n",
      "preparing html data set done.\n"
     ]
    }
   ],
   "source": [
    "# dataset from: https://www.kaggle.com/zavadskyy/lots-of-code, https://gist.github.com/VladislavZavadskyy/e31ab07b03a5c22b11982c49669a400b\n",
    "if data_type in ['all','html']: # parse html\n",
    "    print('prepare html data set...')\n",
    "    with open(os.path.join(output_dir, 'html' + postfix + '.txt'),'w+') as fh:\n",
    "        with open(os.path.join(data_dir, 'html-dataset.txt')) as fp:\n",
    "            data = fp.read()\n",
    "            data = data.replace('<!DOCTYPE html>','\\n<!DOCTYPE html>')\n",
    "            lines = data.split('\\n')\n",
    "            cnt = 0\n",
    "            sample = \"\"\n",
    "            for line in lines:\n",
    "                if line == \"\":\n",
    "                    continue\n",
    "                if sample != \"\" and line.startswith('<!DOCTYPE html>'):\n",
    "                    fh.write(sample.strip() + \"\\n\")\n",
    "                    sample = \"\"\n",
    "                    cnt += 1\n",
    "                if cnt >= num_samples:\n",
    "                    break\n",
    "                line = re.sub(r'\\s+', ' ', line)\n",
    "                sample += line.strip() + \" \"\n",
    "    print('preparing html data set done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Let's fine-tune the GPT-2 model!\n",
    "Choose the number of steps the model will be fine-tuned for. You can adjust the parameters  to specifiy how often you get updates on the training process, how often samples of the current model are printed, and every how many steps the model is saved.\n",
    "\n",
    "Beside the number of steps, these parameters do not influence the training. The model will be saved automatically when done fine-tuning with the amount of steps specified. You can stop the fine-tuning anytime and the current training state of the model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "block:finetune_model",
     "prev:load_music_data"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run fine-tuning for run run1 using GPT2 model 124M...\n",
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
      "2020-06-15 22:04:40,317 [INFO ] [tensorflow  ]: Restoring parameters from models/124M/model.ckpt\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Loading dataset...\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "dataset has 25450 tokens\n",
      "Training...\n",
      "[1 | 91.91] loss=4.13 avg=4.13\n",
      "[2 | 173.89] loss=4.07 avg=4.10\n",
      "[3 | 254.24] loss=3.81 avg=4.00\n",
      "[4 | 334.93] loss=3.81 avg=3.95\n",
      "[5 | 416.09] loss=3.72 avg=3.91\n",
      "[6 | 497.28] loss=3.67 avg=3.87\n",
      "[7 | 578.22] loss=3.58 avg=3.82\n",
      "[8 | 659.74] loss=3.70 avg=3.81\n",
      "[9 | 744.61] loss=3.51 avg=3.77\n",
      "[10 | 828.15] loss=3.42 avg=3.74\n",
      "Saving runs/run1/model-10\n"
     ]
    }
   ],
   "source": [
    "def start_session(sess):\n",
    "    try:\n",
    "        gpt2.reset_session(sess)\n",
    "    except:\n",
    "        pass\n",
    "    return gpt2.start_tf_sess()\n",
    "\n",
    "def fine_tune(sess,run_name,data_path,steps, model_name='124M'):\n",
    "    print(f'Run fine-tuning for run {run_name} using GPT2 model {model_name}...')\n",
    "    if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "        log.info(f\"Downloading {model_name} model...\")\n",
    "        gpt2.download_gpt2(model_name=model_name)\n",
    "    sess = start_session(sess)\n",
    "    gpt2.finetune(sess=sess,dataset=data_path,checkpoint_dir='runs', model_name=model_name, run_name=run_name, steps=steps, sample_every=10, save_every=10)\n",
    "\n",
    "\n",
    "\n",
    "#run_name='run1'\n",
    "#data_path ='data/tweets.txt'\n",
    "sess = None\n",
    "fine_tune(sess,run_name,data_path,stpes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text generation\n",
    "We can now generate text mimiking the style of the learned samples.\n",
    "\n",
    "You can play around with the three parameters `length`, `temperature`, and `top_k` to influnce the generated text. Further, you can provide a seed sequence that will be the beginning of the generated text.\n",
    "\n",
    "Use the different data sets to explore how the fine-tuning works and what its' limits are. You can also use custom data sets. Just copy them to the data folder and specify the path above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "block:generate_text",
     "prev:finetune_model"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint runs/run1/model-10\n",
      "INFO:tensorflow:Restoring parameters from runs/run1/model-10\n",
      "2020-06-15 22:18:36,207 [INFO ] [tensorflow  ]: Restoring parameters from runs/run1/model-10\n",
      "Social justice warior of a British Empire that has been brewing since the 1980s.\n",
      "\n",
      "Island of the People, Canada – B.C.\n",
      "\n",
      "If you're on a flight, please leave your passengers alone.\n",
      "\n",
      "Theresa May, PM: I don't know why anyone would want to torture people. But I have to.\n",
      "\n",
      "We know that there is a good deal of sympathy for the victims of the Paris attacks in their own country, but they're not in the United States.\n",
      "\n",
      "The global financial crisis has put us all on the verge of a disaster.\n",
      "\n",
      "I am deeply disappointed by President Obama's decision today to withdraw the United States from the Paris Agreement.\n",
      "\n",
      "I am absolutely determined to work with the Obama administration to make sure that America is prepared to defend against any and all threats.\n",
      "\n",
      "The United States will not be intimidated into lessening our commitment to North Korea.\n",
      "\n",
      "Europe is committed to using all tools at our disposal to defend itself against any threat.\n",
      "\n",
      "The United States has demonstrated a great commitment to the rule of law and strong international norms of law.\n",
      "\n",
      "The United States has a long history of using torture and other forms of cruel, inhuman and degrading treatment of detainees at Guantánamo Bay to ensure that they are not put in solitary confinement.\n",
      "\n",
      "America's allies in the European Union are very concerned about the US killing of a citizen in the US, and are concerned that the US will use this as leverage to go after terrorists in the Middle East.\n",
      "\n",
      "The United States has a long history of using torture and other forms of cruel, inhuman and degrading treatment of detainees at Guantánamo Bay to ensure that they are not put in solitary confinement. The only reason that the Obama administration refuses to listen to the Senate Intelligence Committee is because they are afraid the US will use these against them.\n",
      "\n",
      "We must find a way to bring the financial crisis to a halt.\n",
      "\n",
      "I will be speaking with the Chinese President on Wednesday, to discuss our response to the crisis in Ukraine.\n",
      "\n",
      "The United States has a long history of using torture and other forms of cruelty to secure our allies' support for the war in Syria.\n",
      "\n",
      "I am extremely concerned that Saudi Arabia is undermining its support for the Syrian rebels.\n",
      "\n",
      "The United States has been a major supporter of the Assad regime and has been extremely supportive of the violence in Syria.\n",
      "\n",
      "The United States will continue to support the Assad government in its fight against ISIS.\n",
      "\n",
      "The United States will continue to be a major supporter of the Libyan government.\n",
      "\n",
      "The United States is a great ally of the United Kingdom and looks forward to working with them in our efforts to secure our borders. In addition, we will be launching a new (and very important) corporate-sponsored border patrol operation.\n",
      "\n",
      "The United States is committed to the people of Yemen. The two countries are a great and good partner.\n",
      "\n",
      "President Obama has made it clear that he would strongly support any action by China or Russia that would invite a military strike on our country. Obama has also said that he would like to put Russia in UN Security Council with China.\n",
      "\n",
      "The United States will continue to work with our allies and partners to fight climate change, including by destroying coal mines and other energy infrastructure.\n",
      "\n",
      "The United States continues to work with partners to fight ISIS in Syria, including by using a vast number of air strikes and other weapons.\n",
      "\n",
      "The United States will continue to work with our European allies and partners to ensure that our European allies don't fall back on irresponsible responses to the crisis in Libya.\n",
      "\n",
      "The United States has a long history of supporting dictatorships that have one of the highest crimes rates in the world.\n",
      "\n",
      "The United States will continue to work closely with our partners to ensure that the United States does not leave the EU.\n",
      "\n",
      "Europe is a very important, and very dangerous, economic and political\n"
     ]
    }
   ],
   "source": [
    "def generate(sess,run_name,length,temperature,top_k):\n",
    "\n",
    "    message = \"Social justice warior\"\n",
    "    text = gpt2.generate(sess=sess,checkpoint_dir='runs', run_name=run_name, prefix=message, length=length, temperature=temperature, top_k=top_k, return_as_list=True)\n",
    "    print(text[0])\n",
    "\n",
    "#length = 800 # { min:0, max:1000, step:5}\n",
    "#temperature = 0.7 # { min:0, max:2, step:0.1}\n",
    "#top_k = 0\n",
    "sess = start_session(sess)\n",
    "gpt2.load_gpt2(sess, checkpoint_dir='runs', run_name=run_name)\n",
    "generate(sess,run_name,length,temperature,top_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "docker_image": "gcr.io/arrikto-public/tensorflow-1.15.2-notebook-cpu:1.0.0.arr1",
   "experiment": {
    "id": "17d9eb89-1f16-4ab5-bc06-9c84d5550990",
    "name": "Gpt2-simple"
   },
   "experiment_name": "Gpt2-simple",
   "pipeline_description": "finetuning basic",
   "pipeline_name": "finetune-pipeline",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "workspace-tb-finetuning-lmy6scgb1",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
